# Data Directory

**Purpose**: Centralized data storage with clear organization and versioning

---

## Directory Structure

```
data/
├── README.md                      # This file
├── raw/                           # Raw API outputs (not in git)
├── processed/                     # Cleaned, analysis-ready datasets
│   └── multi_level_data/          # The reproducibility anchor dataset
├── interim/                       # Intermediate processing outputs
└── external/                      # Third-party datasets
```

---

## Data Organization Philosophy

### Principle 1: Separation of Concerns

- **`raw/`**: Data as received from APIs, never modified
- **`interim/`**: Temporary outputs during processing pipeline
- **`processed/`**: Final, validated datasets ready for analysis
- **`external/`**: Data obtained from third parties (properly cited)

### Principle 2: Immutability

- Raw data is **never edited** after collection
- All transformations are scripted and reproducible
- Processed data includes version numbers and timestamps

### Principle 3: Documentation

- Every dataset has a README explaining its structure
- Data dictionaries define all variables
- Provenance is documented (source, date, processing steps)

---

## Folder Details

### `raw/` - Raw Data

**Status**: Not version-controlled (too large for Git)

**Contents**:
- Congressional Record API responses (JSON/HTML)
- Congress.gov member profiles (JSON)
- Bill metadata (JSON)
- Google Trends data (CSV)

**Storage**:
- Large files stored locally or on institutional server
- Use `.gitignore` to exclude from repository
- Consider DVC (Data Version Control) for large file management

**Naming Convention**:
```
{source}_{congress}_{year}_{type}_{YYYYMMDD}.{ext}

Examples:
- govinfo_114_2015_crec_20241120.json
- congress_gov_members_114_20241120.json
- google_trends_policy_salience_20241120.csv
```

**How to Regenerate**:
```bash
cd pipeline/01_data_collection
python fetch_congressional_records.py --congress 114 --year 2015
python fetch_member_profiles.py --congress 114
python collect_policy_salience.py --congress 114
```

---

### `processed/` - Processed Datasets

**Status**: Selectively version-controlled (small datasets in Git, large via DVC)

**Contents**:
- `multi_level_data/` - The primary reproducibility anchor
- Other processed datasets generated by the pipeline

**Naming Convention**:
```
{descriptive_name}_v{version}_{YYYYMMDD}.{ext}

Examples:
- multi_level_data_v1.0_20241120.parquet
- speaker_assignments_114_115_v2.1_20241125.csv
- classified_mentions_v1.0_20241120.parquet
```

**Version Semantics**:
- **Major version (v1.0 → v2.0)**: Breaking changes to data structure
- **Minor version (v1.0 → v1.1)**: New variables added, backward compatible
- **Patch version (v1.0.0 → v1.0.1)**: Bug fixes, corrections

---

### `processed/multi_level_data/` - The Reproducibility Anchor

**What Is It?**

The `multi_level_data` dataset is a **hierarchical, validated dataset** containing:

- 77,000+ Congressional Record documents (2014-2018)
- Processed paragraphs with speaker assignments
- Interest group mentions with prominence labels
- Machine learning predictions
- Integrated metadata (speaker, group, policy-level variables)

**Why It Exists**:

The original thesis pipeline had manual steps and missing intermediate data. This dataset serves as the **starting point** for reproducible analysis without requiring recreation of the entire legacy workflow.

**Structure**:

```
multi_level_data/
├── README.md                          # Dataset-specific documentation
├── multi_level_data_v1.0.parquet      # Main dataset (Parquet format)
├── data_dictionary_v1.0.csv           # Variable definitions
├── validation_report_v1.0.html        # Quality checks
└── generation_log_v1.0.txt            # Processing log
```

**Quick Start**:

```python
import pandas as pd

# Load the dataset
data = pd.read_parquet('data/processed/multi_level_data/multi_level_data_v1.0.parquet')

# Explore structure
print(data.shape)
print(data.columns)
print(data.head())

# Filter to prominent mentions
prominent = data[data['prominent'] == 1]
```

**See**: `processed/multi_level_data/README.md` for complete documentation

---

### `interim/` - Intermediate Outputs

**Status**: Not version-controlled (reproducible from raw data)

**Contents**:
- Partially processed data between pipeline stages
- Temporary files during long-running computations
- Debugging outputs

**Naming Convention**:
```
{stage}_{descriptive_name}_{YYYYMMDD}.{ext}

Examples:
- 01_parsed_crec_documents_20241120.json
- 02_speaker_assigned_paragraphs_20241120.parquet
- 03_extracted_mentions_20241120.csv
```

**Lifecycle**:
- Generated during pipeline execution
- Can be deleted and regenerated as needed
- Useful for resuming failed pipeline runs

---

### `external/` - Third-Party Data

**Status**: Document source and licensing, may not include raw files

**Contents**:
- Washington Representatives Study data (lobbying, membership)
- Census data for state/district demographics
- CQ Press data on policy areas
- Any manually collected datasets

**Documentation Requirements**:

Each external dataset must have:

1. **Source citation** (file, DOI, URL)
2. **Access date**
3. **License information**
4. **Any modifications made**
5. **Link to original if publicly available**

**Example**:

```
external/
├── washington_representatives_2014_2018/
│   ├── README.md                      # Source and license info
│   ├── WRS_2014.csv                   # Original data
│   ├── WRS_2015.csv
│   ├── WRS_2016.csv
│   ├── WRS_2017.csv
│   ├── WRS_2018.csv
│   └── processing_notes.txt           # Any cleaning steps
```

---

## Data Versioning Strategy

### Approach 1: Manual Versioning (Current)

- Include version number and date in filename
- Document changes in README or CHANGELOG
- Use Git for small datasets (< 50 MB)
- Store large datasets externally with checksums

### Approach 2: DVC (Planned)

**Data Version Control (DVC)** is designed for large datasets:

```bash
# Initialize DVC
dvc init

# Track large file
dvc add data/raw/govinfo_114_2015_crec.json

# Commit the tracking file
git add data/raw/govinfo_114_2015_crec.json.dvc .gitignore
git commit -m "Track raw CREC data with DVC"

# Pull data from remote storage
dvc pull
```

**Benefits**:
- Version large files without bloating Git
- Share data via cloud storage (S3, Google Drive, etc.)
- Reproduce exact dataset versions

**Setup** (planned for Phase 2):
- Configure DVC remote storage
- Migrate existing datasets to DVC
- Document DVC workflow in this README

---

## Data Access Guidelines

### For Users

#### Accessing Processed Data

Most users should start with the `processed/` folder:

```bash
# Clone the repository
git clone https://github.com/kmazurek95/MastersThesis_InterestGroupAnalysis.git
cd MastersThesis_InterestGroupAnalysis

# Access processed data (if small enough to be in Git)
cd data/processed/multi_level_data
ls -lh

# If using DVC
dvc pull data/processed/multi_level_data.dvc
```

#### Accessing Raw Data

Raw data is **not included in the Git repository** due to size. To obtain:

1. **Option A: Request from author**
   - Email kalebmazurek@gmail.com with your use case
   - Large files may be shared via institutional server

2. **Option B: Regenerate from APIs**
   - See `docs/api_setup_guide.md` for API credentials
   - Run data collection scripts in `pipeline/01_data_collection/`

### For Contributors

If you're adding new data to the repository:

1. **Place in appropriate folder** (raw/processed/interim/external)
2. **Follow naming conventions**
3. **Create a README** for the new dataset
4. **Document in data dictionary**
5. **Update this README** with new dataset description
6. **Use DVC** for files > 50 MB

---

## Data Privacy and Ethics

### Congressional Record Data

- **Public Domain**: All Congressional Record text is U.S. Government work
- **No PII**: No private citizen information included
- **Proper Attribution**: Cite GovInfo as source

### External Datasets

- **Respect Licenses**: Check terms before redistribution
- **Cite Sources**: Proper attribution in all publications
- **Embargoes**: Respect any data sharing restrictions

### Ethical Considerations

- Do not identify individual politicians in ways that could be construed as defamatory
- Interest group mentions are factual quotes from public record
- Analysis is descriptive, not normative (we don't endorse groups' positions)

---

## Data Quality Standards

### Validation Checks

All processed datasets should pass:

✅ **Completeness**: No unexpected missing values
✅ **Consistency**: Cross-references match across tables
✅ **Accuracy**: Spot-checks against original sources
✅ **Timeliness**: Timestamps are accurate

### Quality Reports

Each processed dataset should include:

- **Missing data summary**: % missing for each variable
- **Outlier detection**: Flag unusual values
- **Duplicate checks**: Ensure unique identifiers
- **Cross-validation**: Compare with alternative sources

---

## File Format Guidelines

### Recommended Formats

| Data Type | Format | Why |
|-----------|--------|-----|
| Tabular data (< 1 GB) | CSV | Human-readable, widely supported |
| Tabular data (> 1 GB) | Parquet | Compressed, fast, preserves types |
| Nested/hierarchical | JSON | Flexible structure, widely supported |
| Sparse matrices | NPZ (NumPy) | Efficient for ML features |
| Text corpora | JSONL | Streamable, one record per line |

### Avoid

❌ Excel files (.xlsx) - Proprietary, inconsistent behavior
❌ Pickle files (.pkl) - Python-specific, version-sensitive
❌ Uncompressed files when compressed alternative available

---

## Storage and Backup

### Git (< 50 MB)

- Processed datasets small enough to commit directly
- Use `.gitattributes` for LFS if needed

### DVC (> 50 MB)

- Track with DVC, store on remote (S3, GCS, institutional server)
- Commit `.dvc` tracking file to Git

### Institutional Storage

- Large raw data stored on university servers
- Document access procedures in README
- Ensure backups are maintained

### Cloud Storage (Optional)

- Zenodo for long-term archival (with DOI)
- Figshare for public data sharing
- OSF (Open Science Framework) for project data

---

## FAQ

### Q: Where is the `multi_level_data` file?

**A**: See `data/processed/multi_level_data/README.md` for access instructions. If the file is not in the repository, it may be tracked with DVC or available upon request.

### Q: Can I add my own processed datasets?

**A**: Yes! Follow the naming conventions and create a README documenting your dataset. Submit a pull request.

### Q: How do I cite the data?

**A**: See main `README.md` for citation format. Use the DOI if one is assigned.

### Q: What if I find an error in the data?

**A**: Open a GitHub issue with:
- Dataset name and version
- Description of the error
- Steps to reproduce
- Suggested correction

---

## Changelog

### v1.0 (November 2024)

- Initial data directory structure created
- `multi_level_data` dataset documented as reproducibility anchor
- Naming conventions and versioning strategy established

---

## Contact

For questions about data access, quality, or usage:

**Kaleb Mazurek**
Email: kalebmazurek@gmail.com
GitHub: [@kmazurek95](https://github.com/kmazurek95)

---

**Last Updated**: November 25, 2024
